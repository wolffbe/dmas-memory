{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DMAS - Distributed Multi-Agent System\n",
        "\n",
        "Production notebook for managing and testing the DMAS stack with mem0/graphiti memory backends.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "- Set sensitive keys (for example `OPENAI_API_KEY`) manually inside `.env`.\n",
        "- The notebook only refreshes `MEMORY_BACKEND`, `DMAS_RUN_ID`, and inserts placeholders for missing required keys.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded:\n",
            "  Backend: mem0\n",
            "  Run ID: notebook-20251102-150228\n",
            "  Project: /home/jacobbista/Documents/RM&SW/Experiment/dmas-long-context-memory\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import socket\n",
        "from datetime import datetime, timezone\n",
        "from typing import Optional\n",
        "\n",
        "# Backend selection: \"mem0\" or \"graphiti\"\n",
        "MEMORY_BACKEND = \"mem0\"\n",
        "# Experiment run identifier written to .env\n",
        "DMAS_RUN_ID = f\"notebook-{datetime.now(timezone.utc).strftime('%Y%m%d-%H%M%S')}\"\n",
        "\n",
        "# Service URLs\n",
        "LOCOMO_URL = \"http://localhost:8002\"\n",
        "COORDINATOR_URL = \"http://localhost:8003\"\n",
        "MEMORY_URL = \"http://localhost:8005\"\n",
        "RESPONDER_URL = \"http://localhost:8006\"\n",
        "\n",
        "# Paths and required env entries\n",
        "PROJECT_ROOT = os.path.abspath(\".\")\n",
        "ENV_FILE = os.path.join(PROJECT_ROOT, \".env\")\n",
        "REQUIRED_SECRETS = [\"OPENAI_API_KEY\"]\n",
        "\n",
        "# Mirror config into current interpreter environment for local runs\n",
        "os.environ[\"MEMORY_BACKEND\"] = MEMORY_BACKEND\n",
        "os.environ[\"DMAS_RUN_ID\"] = DMAS_RUN_ID\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "print(f\"  Backend: {MEMORY_BACKEND}\")\n",
        "print(f\"  Run ID: {DMAS_RUN_ID}\")\n",
        "print(f\"  Project: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## System Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Functions loaded\n"
          ]
        }
      ],
      "source": [
        "def read_env() -> dict:\n",
        "    \"\"\"Return key/value pairs from .env without exporting them.\"\"\"\n",
        "    env = {}\n",
        "    if os.path.exists(ENV_FILE):\n",
        "        with open(ENV_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line or line.startswith(\"#\") or \"=\" not in line:\n",
        "                    continue\n",
        "                key, value = line.split(\"=\", 1)\n",
        "                env[key.strip()] = value.strip()\n",
        "    return env\n",
        "\n",
        "\n",
        "def write_env(env: dict) -> None:\n",
        "    \"\"\"Persist .env values while preserving manual entries.\"\"\"\n",
        "    lines = [\"# DMAS Configuration\", \"# Managed by notebook where noted\"]\n",
        "    for key, value in sorted(env.items()):\n",
        "        lines.append(f\"{key}={value}\")\n",
        "    with open(ENV_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(lines) + \"\\n\")\n",
        "\n",
        "\n",
        "def update_env_file(backend: str, run_id: str) -> None:\n",
        "    \"\"\"Update runtime knobs; keep user secrets untouched.\"\"\"\n",
        "    env = read_env()\n",
        "    env[\"MEMORY_BACKEND\"] = backend\n",
        "    env[\"DMAS_RUN_ID\"] = run_id\n",
        "    for key in REQUIRED_SECRETS:\n",
        "        env.setdefault(key, f\"<SET_{key}_HERE>\")\n",
        "    write_env(env)\n",
        "    print(f\"‚úÖ .env updated: MEMORY_BACKEND={backend}, DMAS_RUN_ID={run_id}\")\n",
        "\n",
        "\n",
        "def ensure_required_secrets() -> bool:\n",
        "    \"\"\"Warn if critical API keys are missing from env or .env.\"\"\"\n",
        "    env = read_env()\n",
        "    missing = []\n",
        "    for key in REQUIRED_SECRETS:\n",
        "        value = os.getenv(key) or env.get(key)\n",
        "        if not value or value.startswith(\"<SET_\"):\n",
        "            missing.append(key)\n",
        "    if missing:\n",
        "        print(\"‚ùå Missing secrets in environment/.env:\")\n",
        "        for key in missing:\n",
        "            print(f\"  - {key}\")\n",
        "        print(\"Add them manually to .env; the notebook will not write secrets.\")\n",
        "        return False\n",
        "    print(\"‚úÖ Required secrets present\")\n",
        "    return True\n",
        "\n",
        "def check_port_conflicts() -> bool:\n",
        "    \"\"\"Check if required ports are available.\"\"\"\n",
        "    ports = {\n",
        "        8002: \"Locomo\",\n",
        "        8003: \"Coordinator\",\n",
        "        8005: \"Memory\",\n",
        "        8006: \"Responder\",\n",
        "        6333: \"Qdrant\",\n",
        "        7474: \"Neo4j Browser\",\n",
        "        7687: \"Neo4j Bolt\",\n",
        "    }\n",
        "\n",
        "    conflicts = []\n",
        "    for port, service in ports.items():\n",
        "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        sock.settimeout(1)\n",
        "        if sock.connect_ex((\"127.0.0.1\", port)) == 0:\n",
        "            conflicts.append((port, service))\n",
        "        sock.close()\n",
        "\n",
        "    if conflicts:\n",
        "        print(\"‚ùå Port conflicts detected:\")\n",
        "        for port, service in conflicts:\n",
        "            print(f\"  - Port {port} ({service})\")\n",
        "        return False\n",
        "\n",
        "    print(\"‚úÖ All ports available\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def docker_compose(*args: str) -> subprocess.CompletedProcess:\n",
        "    \"\"\"Run docker compose with shared options.\"\"\"\n",
        "    result = subprocess.run(\n",
        "        [\"docker\", \"compose\", *args],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        cwd=PROJECT_ROOT,\n",
        "    )\n",
        "    if result.stdout:\n",
        "        print(result.stdout)\n",
        "    if result.stderr:\n",
        "        print(result.stderr)\n",
        "    return result\n",
        "\n",
        "\n",
        "def docker_up(build: bool = False) -> None:\n",
        "    \"\"\"Start Docker services for the selected profile.\"\"\"\n",
        "    print(f\"Starting services with profile: {MEMORY_BACKEND}\\n\")\n",
        "\n",
        "    args = [\"--profile\", MEMORY_BACKEND, \"up\", \"-d\", \"--remove-orphans\"]\n",
        "    if build:\n",
        "        args.append(\"--build\")\n",
        "    result = docker_compose(*args)\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(\"\\n‚úÖ Services starting (wait ~60s for healthy status)\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Failed: exit code {result.returncode}\")\n",
        "\n",
        "\n",
        "def docker_down() -> None:\n",
        "    \"\"\"Stop any running DMAS services (both profiles).\"\"\"\n",
        "    print(\"Stopping services...\\n\")\n",
        "    result = docker_compose(\n",
        "        \"--profile\",\n",
        "        \"mem0\",\n",
        "        \"--profile\",\n",
        "        \"graphiti\",\n",
        "        \"down\",\n",
        "        \"--remove-orphans\",\n",
        "    )\n",
        "    if result.returncode == 0:\n",
        "        print(\"‚úÖ Services stopped\")\n",
        "\n",
        "\n",
        "def docker_clean() -> None:\n",
        "    \"\"\"Stop services and remove volumes (destructive).\"\"\"\n",
        "    print(\"Cleaning all containers and volumes...\\n\")\n",
        "    result = docker_compose(\n",
        "        \"--profile\",\n",
        "        \"mem0\",\n",
        "        \"--profile\",\n",
        "        \"graphiti\",\n",
        "        \"down\",\n",
        "        \"-v\",\n",
        "        \"--remove-orphans\",\n",
        "    )\n",
        "    if result.returncode == 0:\n",
        "        print(\"‚úÖ Clean complete\")\n",
        "\n",
        "\n",
        "def check_health(url: str, name: str, timeout: int = 5):\n",
        "    \"\"\"Check service health.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{url}/health\", timeout=timeout)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            print(f\"‚úÖ {name:12} - {data.get('status', 'healthy')}\")\n",
        "            return True, data\n",
        "        print(f\"‚ùå {name:12} - HTTP {response.status_code}\")\n",
        "        return False, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {name:12} - {str(e)[:50]}\")\n",
        "        return False, None\n",
        "\n",
        "\n",
        "def check_all_services() -> bool:\n",
        "    \"\"\"Check health of all services.\"\"\"\n",
        "    print(\"Checking services...\\n\")\n",
        "\n",
        "    services = [\n",
        "        (LOCOMO_URL, \"Locomo\"),\n",
        "        (MEMORY_URL, \"Memory\"),\n",
        "        (COORDINATOR_URL, \"Coordinator\"),\n",
        "        (RESPONDER_URL, \"Responder\"),\n",
        "    ]\n",
        "\n",
        "    results = [check_health(url, name) for url, name in services]\n",
        "\n",
        "    if all(r[0] for r in results):\n",
        "        print(\"\\n‚úÖ All services healthy\")\n",
        "        return True\n",
        "    print(\"\\n‚ùå Some services unhealthy\")\n",
        "    return False\n",
        "\n",
        "\n",
        "def memory_stats():\n",
        "    \"\"\"Get memory statistics.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{MEMORY_URL}/stats\")\n",
        "        data = response.json()\n",
        "        print(\"üìä Memory Stats:\")\n",
        "        print(json.dumps(data, indent=2))\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to get stats: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def memory_reset():\n",
        "    \"\"\"Reset memory (clear all stored data).\"\"\"\n",
        "    try:\n",
        "        response = requests.delete(f\"{MEMORY_URL}/reset\")\n",
        "        data = response.json()\n",
        "        print(\"üîÑ Memory Reset:\")\n",
        "        print(json.dumps(data, indent=2))\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to reset: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def verify_memory_state() -> bool:\n",
        "    \"\"\"Verify what's loaded in memory.\"\"\"\n",
        "    print(\"üîç Memory State Verification\\n\")\n",
        "\n",
        "    stats = memory_stats()\n",
        "    if not stats:\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        locomo_response = requests.get(f\"{LOCOMO_URL}/stats\")\n",
        "        locomo_stats = locomo_response.json()\n",
        "        print(f\"\\nüìö Locomo has {locomo_stats['total_conversations']} conversations available\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ö†Ô∏è  Could not get Locomo stats: {e}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def load_conversations(indices=None, poll_interval=3, max_wait_s=900):\n",
        "    \"\"\"\n",
        "    Start the load for each conversation and poll the status.\n",
        "    \"\"\"\n",
        "    indices = list(range(10)) if indices is None else list(indices)\n",
        "    summary = []\n",
        "\n",
        "    for idx in indices:\n",
        "        # 1. get info from locomo\n",
        "        sample_id = None\n",
        "        try:\n",
        "            locomo_resp = requests.get(\n",
        "                f\"{LOCOMO_URL}/conversations/index/{idx}\", timeout=30\n",
        "            )\n",
        "            locomo_resp.raise_for_status()\n",
        "            sample_id = locomo_resp.json().get(\"sample_id\")\n",
        "        except Exception as err:\n",
        "            sample_id = None\n",
        "\n",
        "        # 2. START job on coordinator\n",
        "        try:\n",
        "            start_resp = requests.post(\n",
        "                f\"{COORDINATOR_URL}/load_conversation/index/{idx}\",\n",
        "                timeout=10,\n",
        "            )\n",
        "            start_resp.raise_for_status()\n",
        "            start_data = start_resp.json()\n",
        "            job_id = start_data[\"job_id\"]\n",
        "            memory_job_id = start_data.get(\"memory_job_id\")\n",
        "            \n",
        "            print(f\"[load] idx={idx} started job_id={job_id} (memory_job={memory_job_id or 'n/a'}) sample_id={sample_id or 'unknown'}\")\n",
        "        except Exception as err:\n",
        "            print(f\"[load] idx={idx} failed to start: {err}\")\n",
        "            summary.append({\n",
        "                \"index\": idx,\n",
        "                \"status\": \"error\",\n",
        "                \"sample_id\": sample_id,\n",
        "                \"error\": str(err),\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # 3. Poll until it's done\n",
        "        t0 = time.time()\n",
        "        last_log_len = 0\n",
        "        while True:\n",
        "            if time.time() - t0 > max_wait_s:\n",
        "                print(f\"[load] idx={idx} TIMEOUT after {max_wait_s}s\")\n",
        "                summary.append({\n",
        "                    \"index\": idx,\n",
        "                    \"status\": \"timeout\",\n",
        "                    \"sample_id\": sample_id,\n",
        "                    \"job_id\": job_id,\n",
        "                })\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                st_resp = requests.get(\n",
        "                    f\"{COORDINATOR_URL}/load_conversation/status/{job_id}\",\n",
        "                    timeout=10,\n",
        "                )\n",
        "                st_resp.raise_for_status()\n",
        "                st = st_resp.json()\n",
        "                memory_job_id = st.get(\"memory_job_id\")\n",
        "                if memory_job_id:\n",
        "                    print(f\"[{idx}] memory job is {memory_job_id}\")\n",
        "            except Exception as err:\n",
        "                print(f\"[load] idx={idx} polling error: {err}\")\n",
        "                time.sleep(poll_interval)\n",
        "                continue\n",
        "\n",
        "            logs = st.get(\"logs\") or []\n",
        "            # print only new logs\n",
        "            for line in logs[last_log_len:]:\n",
        "                print(f\"[{idx}] {line}\")\n",
        "            last_log_len = len(logs)\n",
        "\n",
        "            status = st.get(\"status\")\n",
        "            if status in (\"done\", \"error\"):\n",
        "                print(f\"[load] idx={idx} finished status={status}\")\n",
        "                st[\"index\"] = idx\n",
        "                st[\"sample_id\"] = sample_id\n",
        "                summary.append(st)\n",
        "                break\n",
        "\n",
        "            time.sleep(poll_interval)\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def query_memory(prompt: str, conversation_id: Optional[str] = None, limit: Optional[int] = 3):\n",
        "    \"\"\"Helper to call the memory /query endpoint.\"\"\"\n",
        "\n",
        "    payload = {\"prompt\": prompt}\n",
        "    if conversation_id:\n",
        "        payload[\"conversation_id\"] = conversation_id\n",
        "    if limit is not None:\n",
        "        payload[\"limit\"] = limit\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{MEMORY_URL}/query\",\n",
        "            json=payload,\n",
        "            timeout=60,\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "    except Exception as err:  # noqa: BLE001\n",
        "        print(f\"‚ùå Memory query failed: {err}\")\n",
        "        raise\n",
        "\n",
        "    data = response.json()\n",
        "\n",
        "    context_preview = data.get(\"context\")\n",
        "    if context_preview:\n",
        "        print(\"Context preview:\\n\" + context_preview)\n",
        "    else:\n",
        "        print(\"Context preview: <empty>\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "print(\"‚úÖ Functions loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 1: Setup\n",
        "\n",
        "Run these cells in order for initial setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ .env updated: MEMORY_BACKEND=mem0, DMAS_RUN_ID=notebook-20251102-150228\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Update .env file\n",
        "update_env_file(MEMORY_BACKEND, DMAS_RUN_ID)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All ports available\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 317,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 2: Check for port conflicts\n",
        "check_port_conflicts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Required secrets present\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 318,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 3: Verify required secrets before starting services\n",
        "ensure_required_secrets()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: Start Services\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting services with profile: mem0\n",
            "\n",
            "#1 [internal] load local bake definitions\n",
            "#1 reading from stdin 2.35kB done\n",
            "#1 DONE 0.0s\n",
            "\n",
            "#2 [coordinator internal] load build definition from Dockerfile\n",
            "#2 transferring dockerfile: 476B done\n",
            "#2 DONE 0.0s\n",
            "\n",
            "#3 [memory internal] load build definition from Dockerfile\n",
            "#3 transferring dockerfile: 451B done\n",
            "#3 DONE 0.0s\n",
            "\n",
            "#4 [responder internal] load build definition from Dockerfile\n",
            "#4 transferring dockerfile: 466B done\n",
            "#4 DONE 0.0s\n",
            "\n",
            "#5 [locomo internal] load build definition from Dockerfile\n",
            "#5 transferring dockerfile: 360B done\n",
            "#5 DONE 0.0s\n",
            "\n",
            "#6 [responder internal] load metadata for docker.io/library/python:3.11-slim\n",
            "#6 ...\n",
            "\n",
            "#7 [locomo internal] load metadata for docker.io/library/python:3.10-slim\n",
            "#7 DONE 1.3s\n",
            "\n",
            "#6 [memory internal] load metadata for docker.io/library/python:3.11-slim\n",
            "#6 DONE 1.3s\n",
            "\n",
            "#8 [locomo internal] load .dockerignore\n",
            "#8 transferring context: 2B done\n",
            "#8 DONE 0.0s\n",
            "\n",
            "#9 [responder internal] load .dockerignore\n",
            "#9 transferring context: 2B done\n",
            "#9 DONE 0.0s\n",
            "\n",
            "#10 [locomo 1/6] FROM docker.io/library/python:3.10-slim@sha256:e0c4fae70d550834a40f6c3e0326e02cfe239c2351d922e1fb1577a3c6ebde02\n",
            "#10 DONE 0.0s\n",
            "\n",
            "#11 [responder 1/7] FROM docker.io/library/python:3.11-slim@sha256:8eb5fc663972b871c528fef04be4eaa9ab8ab4539a5316c4b8c133771214a617\n",
            "#11 DONE 0.0s\n",
            "\n",
            "#12 [locomo internal] load build context\n",
            "#12 transferring context: 67B done\n",
            "#12 DONE 0.0s\n",
            "\n",
            "#13 [locomo 2/6] WORKDIR /app\n",
            "#13 CACHED\n",
            "\n",
            "#14 [locomo 5/6] RUN pip install --no-cache-dir -r requirements.txt\n",
            "#14 CACHED\n",
            "\n",
            "#15 [locomo 4/6] COPY requirements.txt .\n",
            "#15 CACHED\n",
            "\n",
            "#16 [locomo 3/6] RUN apt-get update &&     apt-get install -y curl &&     apt-get clean &&     rm -rf /var/lib/apt/lists/*\n",
            "#16 CACHED\n",
            "\n",
            "#17 [locomo 6/6] COPY locomo.py .\n",
            "#17 CACHED\n",
            "\n",
            "#18 [coordinator internal] load build context\n",
            "#18 transferring context: 23.41kB done\n",
            "#18 DONE 0.0s\n",
            "\n",
            "#19 [memory internal] load build context\n",
            "#19 transferring context: 287B done\n",
            "#19 DONE 0.0s\n",
            "\n",
            "#20 [responder internal] load build context\n",
            "#20 transferring context: 2.90kB done\n",
            "#20 DONE 0.0s\n",
            "\n",
            "#21 [locomo] exporting to image\n",
            "#21 exporting layers done\n",
            "#21 writing image sha256:74fcc821f6458c8222450473a4455fedc1e4bd15aefb54f8e14762f867f76768 done\n",
            "#21 naming to docker.io/library/dmas-long-context-memory-locomo done\n",
            "#21 DONE 0.0s\n",
            "\n",
            "#22 [coordinator 4/7] COPY coordinator/requirements.txt ./requirements.txt\n",
            "#22 CACHED\n",
            "\n",
            "#23 [coordinator 5/7] RUN pip install --no-cache-dir -r requirements.txt\n",
            "#23 CACHED\n",
            "\n",
            "#24 [coordinator 6/7] COPY dmas_llm /app/dmas_llm\n",
            "#24 CACHED\n",
            "\n",
            "#25 [coordinator 7/7] COPY coordinator/coordinator.py /app/coordinator.py\n",
            "#25 CACHED\n",
            "\n",
            "#26 [responder 4/7] COPY responder/requirements.txt ./requirements.txt\n",
            "#26 CACHED\n",
            "\n",
            "#27 [responder 5/7] RUN pip install --no-cache-dir -r requirements.txt\n",
            "#27 CACHED\n",
            "\n",
            "#28 [responder 6/7] COPY dmas_llm /app/dmas_llm\n",
            "#28 CACHED\n",
            "\n",
            "#29 [responder 7/7] COPY responder/responder.py /app/responder.py\n",
            "#29 CACHED\n",
            "\n",
            "#30 [memory 6/7] COPY dmas_llm /app/dmas_llm\n",
            "#30 CACHED\n",
            "\n",
            "#31 [memory 4/7] COPY memory/requirements.txt ./requirements.txt\n",
            "#31 CACHED\n",
            "\n",
            "#32 [memory 3/7] RUN apt-get update &&     apt-get install -y curl &&     apt-get clean &&     rm -rf /var/lib/apt/lists/*\n",
            "#32 CACHED\n",
            "\n",
            "#33 [memory 5/7] RUN pip install --no-cache-dir -r requirements.txt\n",
            "#33 CACHED\n",
            "\n",
            "#34 [memory 2/7] WORKDIR /app\n",
            "#34 CACHED\n",
            "\n",
            "#35 [memory 7/7] COPY memory/memory.py /app/memory.py\n",
            "#35 CACHED\n",
            "\n",
            "#36 [coordinator] exporting to image\n",
            "#36 exporting layers done\n",
            "#36 writing image sha256:b92230e6efe5a9cab292710f015af7de00dad1d9ecd98ac21714da1520471eb2 done\n",
            "#36 naming to docker.io/library/dmas-long-context-memory-coordinator done\n",
            "#36 DONE 0.0s\n",
            "\n",
            "#37 [responder] exporting to image\n",
            "#37 exporting layers done\n",
            "#37 writing image sha256:abfaad274911c5a0574ff1b7b176a67650a2304bf7a402cd1c2ae95ffacbd500 done\n",
            "#37 naming to docker.io/library/dmas-long-context-memory-responder done\n",
            "#37 DONE 0.0s\n",
            "\n",
            "#38 [memory] exporting to image\n",
            "#38 exporting layers done\n",
            "#38 writing image sha256:78b7d1964705ff26555263004d1b52fbbb4d709a8d3afe2e6e8837623a3b5cf7 done\n",
            "#38 naming to docker.io/library/dmas-long-context-memory-memory done\n",
            "#38 DONE 0.0s\n",
            "\n",
            "#39 [locomo] resolving provenance for metadata file\n",
            "#39 DONE 0.0s\n",
            "\n",
            "#40 [memory] resolving provenance for metadata file\n",
            "#40 DONE 0.0s\n",
            "\n",
            "#41 [responder] resolving provenance for metadata file\n",
            "#41 DONE 0.0s\n",
            "\n",
            "#42 [coordinator] resolving provenance for metadata file\n",
            "#42 DONE 0.0s\n",
            "\n",
            " dmas-long-context-memory-locomo  Built\n",
            " dmas-long-context-memory-memory  Built\n",
            " dmas-long-context-memory-responder  Built\n",
            " dmas-long-context-memory-coordinator  Built\n",
            " Network dmas-long-context-memory_memory-internal  Creating\n",
            " Network dmas-long-context-memory_memory-internal  Created\n",
            " Network dmas-long-context-memory_dmas-network  Creating\n",
            " Network dmas-long-context-memory_dmas-network  Created\n",
            " Container responder  Creating\n",
            " Container locomo  Creating\n",
            " Container qdrant  Creating\n",
            " Container locomo  Created\n",
            " Container qdrant  Created\n",
            " Container responder  Created\n",
            " Container memory  Creating\n",
            " Container memory  Created\n",
            " Container coordinator  Creating\n",
            " Container coordinator  Created\n",
            " Container qdrant  Starting\n",
            " Container responder  Starting\n",
            " Container locomo  Starting\n",
            " Container responder  Started\n",
            " Container qdrant  Started\n",
            " Container qdrant  Waiting\n",
            " Container locomo  Started\n",
            " Container qdrant  Healthy\n",
            " Container memory  Starting\n",
            " Container memory  Started\n",
            " Container memory  Waiting\n",
            " Container responder  Waiting\n",
            " Container locomo  Waiting\n",
            " Container responder  Healthy\n",
            " Container locomo  Healthy\n",
            " Container memory  Healthy\n",
            " Container coordinator  Starting\n",
            " Container coordinator  Started\n",
            "\n",
            "\n",
            "‚úÖ Services starting (wait ~60s for healthy status)\n"
          ]
        }
      ],
      "source": [
        "# Start Docker Compose services\n",
        "docker_up(build=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking services...\n",
            "\n",
            "‚úÖ Locomo       - healthy\n",
            "‚úÖ Memory       - healthy\n",
            "‚úÖ Coordinator  - healthy\n",
            "‚úÖ Responder    - healthy\n",
            "\n",
            "‚úÖ All services healthy\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 320,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Wait 30 sec to 1 minute, then check health\n",
        "time.sleep(5)\n",
        "check_all_services()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 3: Interact with Services\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Locomo Stats:\n",
            "{\n",
            "  \"total_conversations\": 10,\n",
            "  \"total_sessions\": 272,\n",
            "  \"total_turns\": 5882,\n",
            "  \"total_questions\": 1986,\n",
            "  \"conversations_loaded\": true,\n",
            "  \"data_file\": \"/data/locomo10.json\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Get Locomo stats\n",
        "response = requests.get(f\"{LOCOMO_URL}/stats\")\n",
        "stats = response.json()\n",
        "print(\"üìä Locomo Stats:\")\n",
        "print(json.dumps(stats, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Memory Stats:\n",
            "{\n",
            "  \"status\": \"success\",\n",
            "  \"backend\": \"mem0\",\n",
            "  \"collection\": \"conversations\",\n",
            "  \"qdrant_host\": \"qdrant\",\n",
            "  \"qdrant_port\": 6333,\n",
            "  \"note\": \"Memory is operational. Use Qdrant API directly for detailed stats.\",\n",
            "  \"test_query_success\": true\n",
            "}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'status': 'success',\n",
              " 'backend': 'mem0',\n",
              " 'collection': 'conversations',\n",
              " 'qdrant_host': 'qdrant',\n",
              " 'qdrant_port': 6333,\n",
              " 'note': 'Memory is operational. Use Qdrant API directly for detailed stats.',\n",
              " 'test_query_success': True}"
            ]
          },
          "execution_count": 322,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get memory stats\n",
        "memory_stats()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load conversations into memory via coordinator this may take a while (300s)\n",
        "TARGET_INDEX = 0\n",
        "load_summary = load_conversations(indices=[TARGET_INDEX])\n",
        "load_summary\n",
        "\n",
        "sample_id = next(\n",
        "    (item.get(\"sample_id\") for item in load_summary if item.get(\"sample_id\")),\n",
        "    None,\n",
        ")\n",
        "print(f\"Selected sample_id: {sample_id or 'none'}\")\n",
        "\n",
        "sample_prompt = \"Summarize the key events in the conversation.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Preview for: Summarize the key events in the conversation.\n",
            "   Conversation ID: conv-26\n",
            "   Memory chunks: 3\n",
            "\n",
            "üì¶ RAW context:\n",
            "   Tokens: 93\n",
            "   Chars: 301\n",
            "   Text: [session_10 | 8:56 pm on 20 July, 2023] Participated in regular meetings, events, and campaigns with the group\n",
            "\n",
            "[session_3 | 7:55 pm on 9 June, 2023] ...\n",
            "\n",
            "üìâ CONDENSED context:\n",
            "   Model: gpt-4o-mini\n",
            "   Tokens: 80\n",
            "   Chars: 330\n",
            "   Compression: 86.0%\n",
            "   Summary: Key events in the conversation include:\n",
            "\n",
            "- On July 20, 2023, the participant engaged in regular meetings, events, and campaigns with the group.\n",
            "- On J...\n"
          ]
        }
      ],
      "source": [
        "# Preview condensed context via coordinator (raw vs condensed token counts)\n",
        "if not sample_id:\n",
        "    print(\"‚ö†Ô∏è Cannot preview without a conversation_id.\")\n",
        "else:\n",
        "    preview_payload = {\n",
        "        \"question\": sample_prompt,\n",
        "        \"limit\": 3,\n",
        "        \"conversation_id\": sample_id,\n",
        "    }\n",
        "\n",
        "    preview_response = requests.post(\n",
        "        f\"{COORDINATOR_URL}/preview\",\n",
        "        json=preview_payload,\n",
        "        timeout=60,\n",
        "    )\n",
        "    preview_response.raise_for_status()\n",
        "    preview_data = preview_response.json()\n",
        "\n",
        "    raw_info = preview_data.get(\"raw\", {})\n",
        "    condensed_info = preview_data.get(\"condensed\", {})\n",
        "    memory_info = preview_data.get(\"memory\", {})\n",
        "\n",
        "    print(f\"üîç Preview for: {sample_prompt}\")\n",
        "    print(f\"   Conversation ID: {sample_id}\")\n",
        "    print(f\"   Memory chunks: {memory_info.get('count', 0)}\\n\")\n",
        "    \n",
        "    print(f\"üì¶ RAW context:\")\n",
        "    print(f\"   Tokens: {raw_info.get('tokens', 0)}\")\n",
        "    print(f\"   Chars: {raw_info.get('characters', 0)}\")\n",
        "    raw_ctx = raw_info.get('context') or ''\n",
        "    print(f\"   Text: {raw_ctx[:150]}...\\n\" if raw_ctx else \"   Text: <empty>\\n\")\n",
        "    \n",
        "    print(f\"üìâ CONDENSED context:\")\n",
        "    print(f\"   Model: {condensed_info.get('model', 'N/A')}\")\n",
        "    print(f\"   Tokens: {condensed_info.get('tokens', 0)}\")\n",
        "    print(f\"   Chars: {condensed_info.get('characters', 0)}\")\n",
        "    ratio = condensed_info.get('compression_ratio')\n",
        "    if ratio:\n",
        "        print(f\"   Compression: {ratio:.1%}\")\n",
        "    summary = condensed_info.get('summary') or ''\n",
        "    print(f\"   Summary: {summary[:150]}...\" if summary else \"   Summary: <empty>\")\n",
        "    \n",
        "    preview_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ground-truth question: When did Caroline go to the LGBTQ support group?\n",
            "Ground-truth answer: 7 May 2023\n"
          ]
        }
      ],
      "source": [
        "# Fetch first Locomo QA pair for evaluation\n",
        "qa_question = None\n",
        "qa_answer = None\n",
        "if sample_id is None:\n",
        "    print(\"‚ö†Ô∏è Cannot fetch QA without a sample_id.\")\n",
        "else:\n",
        "    try:\n",
        "        qa_response = requests.get(\n",
        "            f\"{LOCOMO_URL}/conversations/index/{TARGET_INDEX}/questions\",\n",
        "            timeout=60,\n",
        "        )\n",
        "        qa_response.raise_for_status()\n",
        "        qa_payload = qa_response.json()\n",
        "        questions = qa_payload.get(\"questions\") or []\n",
        "        if not questions:\n",
        "            print(\"‚ö†Ô∏è No QA entries found for this conversation.\")\n",
        "        else:\n",
        "            qa_entry = questions[0]\n",
        "            qa_question = qa_entry.get(\"question\")\n",
        "            qa_answer = qa_entry.get(\"answer\") or qa_entry.get(\"adversarial_answer\")\n",
        "            print(\"Ground-truth question:\", qa_question)\n",
        "            print(\"Ground-truth answer:\", qa_answer)\n",
        "    except Exception as err:  # noqa: BLE001\n",
        "        print(f\"‚ùå Failed to fetch QA: {err}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context preview:\n",
            "[session_14 | 1:33 pm on 25 August, 2023] Caroline is part of the transgender community\n",
            "\n",
            "[session_8 | 1:51 pm on 15 July, 2023] The support group has made Caroline feel accepted and given her courage to embrace herself.\n",
            "\n",
            "[session_14 | 1:33 pm on 25 August, 2023] Caroline is putting together an LGBTQ art show next month to showcase her paintings and feature LGBTQ artists.\n"
          ]
        }
      ],
      "source": [
        "# Query memory manually to inspect retrieved context for the first QA\n",
        "if not sample_id or not qa_question:\n",
        "    print(\"‚ö†Ô∏è Missing sample_id or QA question.\")\n",
        "else:\n",
        "    memory_response = query_memory(qa_question, conversation_id=sample_id, limit=10)\n",
        "    memory_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask a question end-to-end via coordinator/responder\n",
        "if not qa_question:\n",
        "    print(\"‚ö†Ô∏è No QA question available to ask.\")\n",
        "else:\n",
        "    question = qa_question\n",
        "    print(f\"Question: {question}\\n\")\n",
        "\n",
        "    ask_payload = {\"question\": question}\n",
        "    if sample_id:\n",
        "        ask_payload[\"conversation_id\"] = sample_id\n",
        "        ask_payload[\"limit\"] = 15\n",
        "\n",
        "    response = requests.post(\n",
        "        f\"{COORDINATOR_URL}/ask\",\n",
        "        json=ask_payload,\n",
        "        timeout=60\n",
        "    )\n",
        "\n",
        "    result = response.json()\n",
        "    print(\"Answer:\")\n",
        "    print(json.dumps(result, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare responder answer with ground truth\n",
        "if qa_question and qa_answer and result.get(\"status\") == \"success\":\n",
        "    responder_answer = result.get(\"answer\") or \"\"\n",
        "    print(\"Ground truth:\", qa_answer)\n",
        "    print(\"Responder:\", responder_answer)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Unable to compute evaluation (missing data or responder error).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 4: Memory Management & Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify memory state\n",
        "verify_memory_state()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset memory (clear all data)\n",
        "memory_reset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify memory cleared\n",
        "memory_stats()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 5: Shutdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stopping services...\n",
            "\n",
            " Container coordinator  Stopping\n",
            " Container coordinator  Stopped\n",
            " Container coordinator  Removing\n",
            " Container coordinator  Removed\n",
            " Container memory  Stopping\n",
            " Container locomo  Stopping\n",
            " Container responder  Stopping\n",
            " Container locomo  Stopped\n",
            " Container locomo  Removing\n",
            " Container locomo  Removed\n",
            " Container responder  Stopped\n",
            " Container responder  Removing\n",
            " Container responder  Removed\n",
            " Container memory  Stopped\n",
            " Container memory  Removing\n",
            " Container memory  Removed\n",
            " Container qdrant  Stopping\n",
            " Container qdrant  Stopped\n",
            " Container qdrant  Removing\n",
            " Container qdrant  Removed\n",
            " Network dmas-long-context-memory_memory-internal  Removing\n",
            " Network dmas-long-context-memory_dmas-network  Removing\n",
            " Network dmas-long-context-memory_memory-internal  Removed\n",
            " Network dmas-long-context-memory_dmas-network  Removed\n",
            "\n",
            "‚úÖ Services stopped\n"
          ]
        }
      ],
      "source": [
        "# Stop services (preserves data volumes)\n",
        "docker_down()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTIONAL: Clean everything (removes all volumes)\n",
        "# Uncomment to use:\n",
        "# docker_clean()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
