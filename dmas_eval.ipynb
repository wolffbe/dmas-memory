{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de74126d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# DMAS Long-Context Memory Evaluation Notebook\n",
    "# This notebook evaluates the accuracy and cost of long-context vector vs graph memory\n",
    "# in distributed LLM-based multi-agent systems\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "LOCOMO_URL = \"http://localhost:8002\"\n",
    "COORDINATOR_URL = \"http://localhost:8003\"\n",
    "MEMORY_URL = \"http://localhost:8005\"\n",
    "\n",
    "print(\"üìä DMAS Long-Context Memory Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Locomo URL: {LOCOMO_URL}\")\n",
    "print(f\"Coordinator URL: {COORDINATOR_URL}\")\n",
    "print(f\"Memory URL: {MEMORY_URL}\")\n",
    "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe9309",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions for API Communication\n",
    "\n",
    "def check_service_health(url: str, service_name: str) -> bool:\n",
    "    \"\"\"Check if a service is healthy\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{url}/health\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ {service_name} is healthy\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå {service_name} returned status {response.status_code}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {service_name} is not reachable: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_conversations_from_locomo(conv_index: Optional[int] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Load conversations from locomo service\"\"\"\n",
    "    try:\n",
    "        if conv_index is not None:\n",
    "            url = f\"{LOCOMO_URL}/conversations/index/{conv_index}\"\n",
    "        else:\n",
    "            url = f\"{LOCOMO_URL}/conversations\"\n",
    "        \n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading conversations: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def load_questions_from_locomo(conv_index: Optional[int] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Load questions from locomo service\"\"\"\n",
    "    try:\n",
    "        if conv_index is not None:\n",
    "            url = f\"{LOCOMO_URL}/conversations/index/{conv_index}/questions\"\n",
    "        else:\n",
    "            # Get all conversations first, then extract questions\n",
    "            conversations = load_conversations_from_locomo()\n",
    "            all_questions = []\n",
    "            if \"conversations\" in conversations:\n",
    "                for conv in conversations[\"conversations\"]:\n",
    "                    sample_id = conv.get(\"sample_id\")\n",
    "                    if sample_id:\n",
    "                        questions_response = requests.get(f\"{LOCOMO_URL}/conversations/{sample_id}/questions\", timeout=30)\n",
    "                        if questions_response.status_code == 200:\n",
    "                            questions_data = questions_response.json()\n",
    "                            all_questions.extend(questions_data.get(\"questions\", []))\n",
    "            return {\"questions\": all_questions}\n",
    "        \n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading questions: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def ask_question_via_coordinator(question: str) -> Dict[str, Any]:\n",
    "    \"\"\"Ask a question via the coordinator service\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{COORDINATOR_URL}/ask\",\n",
    "            json={\"question\": question},\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error asking question: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Check all services\n",
    "print(\"üîç Checking service health...\")\n",
    "services_healthy = {\n",
    "    \"locomo\": check_service_health(LOCOMO_URL, \"Locomo\"),\n",
    "    \"coordinator\": check_service_health(COORDINATOR_URL, \"Coordinator\"),\n",
    "    \"memory\": check_service_health(MEMORY_URL, \"Memory\")\n",
    "}\n",
    "\n",
    "if all(services_healthy.values()):\n",
    "    print(\"‚úÖ All services are healthy!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some services are not healthy. Please check docker-compose status.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69180460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Conversations from Locomo\n",
    "print(\"üìö Loading conversations from Locomo...\")\n",
    "\n",
    "# Load all conversations\n",
    "conversations_data = load_conversations_from_locomo()\n",
    "if \"error\" not in conversations_data:\n",
    "    print(f\"‚úÖ Loaded {conversations_data.get('total', 0)} conversations\")\n",
    "    \n",
    "    # Display conversation summary\n",
    "    if \"conversations\" in conversations_data:\n",
    "        conv_summary = []\n",
    "        for i, conv in enumerate(conversations_data[\"conversations\"]):\n",
    "            conv_summary.append({\n",
    "                \"index\": i,\n",
    "                \"sample_id\": conv.get(\"sample_id\", \"N/A\"),\n",
    "                \"speaker_a\": conv.get(\"speaker_a\", \"N/A\"),\n",
    "                \"speaker_b\": conv.get(\"speaker_b\", \"N/A\"),\n",
    "                \"sessions_count\": len([k for k in conv.get(\"sessions\", {}).keys() if k.startswith(\"session_\")])\n",
    "            })\n",
    "        \n",
    "        conv_df = pd.DataFrame(conv_summary)\n",
    "        print(\"\\nüìã Conversation Summary:\")\n",
    "        print(conv_df.to_string(index=False))\n",
    "        \n",
    "        # Store conversations for later use\n",
    "        conversations = conversations_data[\"conversations\"]\n",
    "    else:\n",
    "        conversations = []\n",
    "        print(\"‚ö†Ô∏è No conversations found in response\")\n",
    "else:\n",
    "    print(f\"‚ùå Failed to load conversations: {conversations_data['error']}\")\n",
    "    conversations = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56466d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Questions from Locomo\n",
    "print(\"‚ùì Loading questions from Locomo...\")\n",
    "\n",
    "# Load all questions\n",
    "questions_data = load_questions_from_locomo()\n",
    "if \"error\" not in questions_data:\n",
    "    questions = questions_data.get(\"questions\", [])\n",
    "    print(f\"‚úÖ Loaded {len(questions)} questions\")\n",
    "    \n",
    "    if questions:\n",
    "        # Display question summary\n",
    "        question_summary = []\n",
    "        for i, q in enumerate(questions[:10]):  # Show first 10 questions\n",
    "            question_summary.append({\n",
    "                \"index\": i,\n",
    "                \"sample_id\": q.get(\"sample_id\", \"N/A\"),\n",
    "                \"question\": q.get(\"question\", \"N/A\")[:50] + \"...\" if len(q.get(\"question\", \"\")) > 50 else q.get(\"question\", \"N/A\"),\n",
    "                \"answer\": q.get(\"answer\", \"N/A\")[:30] + \"...\" if len(q.get(\"answer\", \"\")) > 30 else q.get(\"answer\", \"N/A\"),\n",
    "                \"category\": q.get(\"category\", \"N/A\")\n",
    "            })\n",
    "        \n",
    "        questions_df = pd.DataFrame(question_summary)\n",
    "        print(\"\\n‚ùì Questions Summary (first 10):\")\n",
    "        print(questions_df.to_string(index=False))\n",
    "        \n",
    "        # Analyze question categories\n",
    "        categories = [q.get(\"category\") for q in questions if q.get(\"category\")]\n",
    "        if categories:\n",
    "            category_counts = pd.Series(categories).value_counts()\n",
    "            print(f\"\\nüìä Question Categories:\")\n",
    "            print(category_counts.to_string())\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No questions found\")\n",
    "else:\n",
    "    print(f\"‚ùå Failed to load questions: {questions_data['error']}\")\n",
    "    questions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad68ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Textual Similarity Evaluation\n",
    "print(\"üéØ Implementing F1 Textual Similarity Evaluation...\")\n",
    "\n",
    "# Initialize sentence transformer for semantic similarity\n",
    "try:\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print(\"‚úÖ Sentence transformer model loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading sentence transformer: {e}\")\n",
    "    model = None\n",
    "\n",
    "def calculate_textual_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two texts\"\"\"\n",
    "    if not model or not text1 or not text2:\n",
    "        return 0.0\n",
    "    \n",
    "    try:\n",
    "        embeddings = model.encode([text1, text2])\n",
    "        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "        return float(similarity)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating similarity: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def evaluate_answer_similarity(predicted_answer: str, ground_truth_answer: str) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate similarity between predicted and ground truth answers\"\"\"\n",
    "    if not predicted_answer or not ground_truth_answer:\n",
    "        return {\"similarity\": 0.0, \"f1_score\": 0.0}\n",
    "    \n",
    "    # Calculate semantic similarity\n",
    "    semantic_similarity = calculate_textual_similarity(predicted_answer, ground_truth_answer)\n",
    "    \n",
    "    # For F1 score, we'll use a threshold-based approach\n",
    "    # Convert similarity to binary classification (similar/not similar)\n",
    "    threshold = 0.7  # Adjust this threshold as needed\n",
    "    is_similar = semantic_similarity >= threshold\n",
    "    \n",
    "    # For demonstration, we'll use semantic similarity as a proxy for F1\n",
    "    # In a real scenario, you might want to use more sophisticated metrics\n",
    "    f1_proxy = semantic_similarity  # This is a simplified approach\n",
    "    \n",
    "    return {\n",
    "        \"similarity\": semantic_similarity,\n",
    "        \"f1_score\": f1_proxy,\n",
    "        \"is_similar\": is_similar,\n",
    "        \"threshold\": threshold\n",
    "    }\n",
    "\n",
    "def run_evaluation_on_questions(questions: List[Dict], max_questions: int = 5) -> List[Dict]:\n",
    "    \"\"\"Run evaluation on a subset of questions\"\"\"\n",
    "    if not questions:\n",
    "        print(\"‚ö†Ô∏è No questions available for evaluation\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üîÑ Running evaluation on {min(max_questions, len(questions))} questions...\")\n",
    "    \n",
    "    results = []\n",
    "    for i, question_data in enumerate(questions[:max_questions]):\n",
    "        question = question_data.get(\"question\", \"\")\n",
    "        ground_truth = question_data.get(\"answer\", \"\")\n",
    "        \n",
    "        if not question or not ground_truth:\n",
    "            print(f\"‚ö†Ô∏è Skipping question {i}: missing question or answer\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"üìù Question {i+1}: {question[:50]}...\")\n",
    "        \n",
    "        # Ask the question via coordinator\n",
    "        start_time = time.time()\n",
    "        response = ask_question_via_coordinator(question)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if \"error\" in response:\n",
    "            print(f\"‚ùå Error asking question {i+1}: {response['error']}\")\n",
    "            continue\n",
    "        \n",
    "        predicted_answer = response.get(\"answer\", \"\")\n",
    "        \n",
    "        # Evaluate similarity\n",
    "        evaluation = evaluate_answer_similarity(predicted_answer, ground_truth)\n",
    "        \n",
    "        result = {\n",
    "            \"question_index\": i,\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"response_time\": end_time - start_time,\n",
    "            **evaluation\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"‚úÖ Question {i+1} completed - Similarity: {evaluation['similarity']:.3f}\")\n",
    "        time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation if we have questions\n",
    "if questions and model:\n",
    "    evaluation_results = run_evaluation_on_questions(questions, max_questions=3)\n",
    "    \n",
    "    if evaluation_results:\n",
    "        print(f\"\\nüìä Evaluation Results Summary:\")\n",
    "        similarities = [r[\"similarity\"] for r in evaluation_results]\n",
    "        response_times = [r[\"response_time\"] for r in evaluation_results]\n",
    "        \n",
    "        print(f\"Average Similarity: {np.mean(similarities):.3f}\")\n",
    "        print(f\"Average Response Time: {np.mean(response_times):.2f}s\")\n",
    "        print(f\"Max Similarity: {np.max(similarities):.3f}\")\n",
    "        print(f\"Min Similarity: {np.min(similarities):.3f}\")\n",
    "        \n",
    "        # Store results for visualization\n",
    "        eval_df = pd.DataFrame(evaluation_results)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No evaluation results generated\")\n",
    "        eval_df = pd.DataFrame()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot run evaluation - missing questions or model\")\n",
    "    eval_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b8407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization and Analysis\n",
    "print(\"üìà Creating visualizations and analysis...\")\n",
    "\n",
    "if not eval_df.empty:\n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('DMAS Long-Context Memory Evaluation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Similarity Distribution\n",
    "    axes[0, 0].hist(eval_df['similarity'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Similarity Score Distribution')\n",
    "    axes[0, 0].set_xlabel('Similarity Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].axvline(eval_df['similarity'].mean(), color='red', linestyle='--', \n",
    "                      label=f'Mean: {eval_df[\"similarity\"].mean():.3f}')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Response Time Analysis\n",
    "    axes[0, 1].bar(range(len(eval_df)), eval_df['response_time'], color='lightcoral', alpha=0.7)\n",
    "    axes[0, 1].set_title('Response Time per Question')\n",
    "    axes[0, 1].set_xlabel('Question Index')\n",
    "    axes[0, 1].set_ylabel('Response Time (seconds)')\n",
    "    axes[0, 1].axhline(eval_df['response_time'].mean(), color='red', linestyle='--',\n",
    "                      label=f'Mean: {eval_df[\"response_time\"].mean():.2f}s')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Similarity vs Response Time Scatter\n",
    "    axes[1, 0].scatter(eval_df['response_time'], eval_df['similarity'], \n",
    "                      alpha=0.7, s=100, color='green')\n",
    "    axes[1, 0].set_title('Similarity vs Response Time')\n",
    "    axes[1, 0].set_xlabel('Response Time (seconds)')\n",
    "    axes[1, 0].set_ylabel('Similarity Score')\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    corr = eval_df['response_time'].corr(eval_df['similarity'])\n",
    "    axes[1, 0].text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "                    transform=axes[1, 0].transAxes, fontsize=10,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    # 4. Performance Summary\n",
    "    axes[1, 1].axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    üìä Performance Summary\n",
    "    \n",
    "    Total Questions Evaluated: {len(eval_df)}\n",
    "    \n",
    "    Average Similarity: {eval_df['similarity'].mean():.3f}\n",
    "    Max Similarity: {eval_df['similarity'].max():.3f}\n",
    "    Min Similarity: {eval_df['similarity'].min():.3f}\n",
    "    \n",
    "    Average Response Time: {eval_df['response_time'].mean():.2f}s\n",
    "    Max Response Time: {eval_df['response_time'].max():.2f}s\n",
    "    Min Response Time: {eval_df['response_time'].min():.2f}s\n",
    "    \n",
    "    Questions Above Threshold: {sum(eval_df['is_similar'])}/{len(eval_df)}\n",
    "    Success Rate: {sum(eval_df['is_similar'])/len(eval_df)*100:.1f}%\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.9, summary_text, transform=axes[1, 1].transAxes,\n",
    "                    fontsize=11, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed results table\n",
    "    print(\"\\nüìã Detailed Evaluation Results:\")\n",
    "    display_cols = ['question_index', 'similarity', 'response_time', 'is_similar']\n",
    "    print(eval_df[display_cols].to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No evaluation data available for visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b474dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit Testing for Memory System\n",
    "print(\"üß™ Implementing Unit Tests for Memory System...\")\n",
    "\n",
    "import unittest\n",
    "from unittest.mock import Mock, patch\n",
    "import asyncio\n",
    "\n",
    "class MemorySystemTests(unittest.TestCase):\n",
    "    \"\"\"Unit tests for the memory system\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Set up test fixtures\"\"\"\n",
    "        self.test_conversation_data = {\n",
    "            \"sample_id\": \"test_conv_001\",\n",
    "            \"speaker_a\": \"Alice\",\n",
    "            \"speaker_b\": \"Bob\",\n",
    "            \"sessions\": {\n",
    "                \"session_1\": [\n",
    "                    {\"speaker\": \"Alice\", \"text\": \"Hello Bob, how are you?\", \"dia_id\": \"1\"},\n",
    "                    {\"speaker\": \"Bob\", \"text\": \"I'm doing well, thanks!\", \"dia_id\": \"2\"}\n",
    "                ]\n",
    "            },\n",
    "            \"session_datetimes\": {\n",
    "                \"session_1_date_time\": \"10:00 AM on 1 January, 2024\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.test_questions = [\n",
    "            {\n",
    "                \"question\": \"How is Bob doing?\",\n",
    "                \"answer\": \"Bob is doing well\",\n",
    "                \"category\": \"emotional_state\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"Who spoke first?\",\n",
    "                \"answer\": \"Alice spoke first\",\n",
    "                \"category\": \"speaker_identification\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def test_conversation_data_structure(self):\n",
    "        \"\"\"Test that conversation data has required structure\"\"\"\n",
    "        self.assertIn(\"sample_id\", self.test_conversation_data)\n",
    "        self.assertIn(\"sessions\", self.test_conversation_data)\n",
    "        self.assertIn(\"session_datetimes\", self.test_conversation_data)\n",
    "        \n",
    "        # Check session structure\n",
    "        sessions = self.test_conversation_data[\"sessions\"]\n",
    "        self.assertIsInstance(sessions, dict)\n",
    "        self.assertIn(\"session_1\", sessions)\n",
    "        \n",
    "        # Check session content\n",
    "        session_1 = sessions[\"session_1\"]\n",
    "        self.assertIsInstance(session_1, list)\n",
    "        self.assertEqual(len(session_1), 2)\n",
    "        \n",
    "        # Check turn structure\n",
    "        turn = session_1[0]\n",
    "        self.assertIn(\"speaker\", turn)\n",
    "        self.assertIn(\"text\", turn)\n",
    "        self.assertIn(\"dia_id\", turn)\n",
    "    \n",
    "    def test_question_data_structure(self):\n",
    "        \"\"\"Test that question data has required structure\"\"\"\n",
    "        for question in self.test_questions:\n",
    "            self.assertIn(\"question\", question)\n",
    "            self.assertIn(\"answer\", question)\n",
    "            self.assertIn(\"category\", question)\n",
    "            \n",
    "            # Check that fields are not empty\n",
    "            self.assertTrue(question[\"question\"].strip())\n",
    "            self.assertTrue(question[\"answer\"].strip())\n",
    "    \n",
    "    def test_similarity_calculation(self):\n",
    "        \"\"\"Test similarity calculation function\"\"\"\n",
    "        # Test with identical texts\n",
    "        similarity = calculate_textual_similarity(\"Hello world\", \"Hello world\")\n",
    "        self.assertAlmostEqual(similarity, 1.0, places=2)\n",
    "        \n",
    "        # Test with completely different texts\n",
    "        similarity = calculate_textual_similarity(\"Hello world\", \"xyz abc def\")\n",
    "        self.assertLess(similarity, 0.5)\n",
    "        \n",
    "        # Test with empty texts\n",
    "        similarity = calculate_textual_similarity(\"\", \"Hello world\")\n",
    "        self.assertEqual(similarity, 0.0)\n",
    "    \n",
    "    def test_evaluation_metrics(self):\n",
    "        \"\"\"Test evaluation metrics calculation\"\"\"\n",
    "        # Test with identical answers\n",
    "        evaluation = evaluate_answer_similarity(\"Bob is doing well\", \"Bob is doing well\")\n",
    "        self.assertGreater(evaluation[\"similarity\"], 0.9)\n",
    "        self.assertTrue(evaluation[\"is_similar\"])\n",
    "        \n",
    "        # Test with different answers\n",
    "        evaluation = evaluate_answer_similarity(\"Bob is doing well\", \"Alice is happy\")\n",
    "        self.assertLess(evaluation[\"similarity\"], 0.8)\n",
    "    \n",
    "    @patch('requests.post')\n",
    "    def test_api_error_handling(self, mock_post):\n",
    "        \"\"\"Test API error handling\"\"\"\n",
    "        # Mock API error response\n",
    "        mock_response = Mock()\n",
    "        mock_response.status_code = 500\n",
    "        mock_response.raise_for_status.side_effect = requests.exceptions.HTTPError(\"Server Error\")\n",
    "        mock_post.return_value = mock_response\n",
    "        \n",
    "        # Test that error is handled gracefully\n",
    "        result = ask_question_via_coordinator(\"Test question\")\n",
    "        self.assertIn(\"error\", result)\n",
    "    \n",
    "    def test_data_validation(self):\n",
    "        \"\"\"Test data validation functions\"\"\"\n",
    "        # Test valid conversation data\n",
    "        self.assertTrue(self._is_valid_conversation_data(self.test_conversation_data))\n",
    "        \n",
    "        # Test invalid conversation data\n",
    "        invalid_data = {\"sample_id\": \"test\"}\n",
    "        self.assertFalse(self._is_valid_conversation_data(invalid_data))\n",
    "    \n",
    "    def _is_valid_conversation_data(self, data):\n",
    "        \"\"\"Helper function to validate conversation data\"\"\"\n",
    "        required_fields = [\"sample_id\", \"sessions\", \"session_datetimes\"]\n",
    "        return all(field in data for field in required_fields)\n",
    "\n",
    "def run_memory_tests():\n",
    "    \"\"\"Run all memory system tests\"\"\"\n",
    "    print(\"üß™ Running Memory System Unit Tests...\")\n",
    "    \n",
    "    # Create test suite\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(MemorySystemTests)\n",
    "    \n",
    "    # Run tests\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    result = runner.run(suite)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nüìä Test Results Summary:\")\n",
    "    print(f\"Tests run: {result.testsRun}\")\n",
    "    print(f\"Failures: {len(result.failures)}\")\n",
    "    print(f\"Errors: {len(result.errors)}\")\n",
    "    print(f\"Success rate: {((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100):.1f}%\")\n",
    "    \n",
    "    if result.failures:\n",
    "        print(\"\\n‚ùå Test Failures:\")\n",
    "        for test, traceback in result.failures:\n",
    "            print(f\"- {test}: {traceback}\")\n",
    "    \n",
    "    if result.errors:\n",
    "        print(\"\\n‚ùå Test Errors:\")\n",
    "        for test, traceback in result.errors:\n",
    "            print(f\"- {test}: {traceback}\")\n",
    "    \n",
    "    return result.wasSuccessful()\n",
    "\n",
    "# Run the tests\n",
    "test_success = run_memory_tests()\n",
    "\n",
    "if test_success:\n",
    "    print(\"‚úÖ All memory system tests passed!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some memory system tests failed. Check the output above for details.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c092286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Results and Generate Report\n",
    "print(\"üìÑ Generating comprehensive evaluation report...\")\n",
    "\n",
    "def generate_evaluation_report():\n",
    "    \"\"\"Generate a comprehensive evaluation report\"\"\"\n",
    "    report = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"system_info\": {\n",
    "            \"locomo_url\": LOCOMO_URL,\n",
    "            \"coordinator_url\": COORDINATOR_URL,\n",
    "            \"memory_url\": MEMORY_URL\n",
    "        },\n",
    "        \"data_summary\": {\n",
    "            \"total_conversations\": len(conversations) if conversations else 0,\n",
    "            \"total_questions\": len(questions) if questions else 0,\n",
    "            \"services_healthy\": services_healthy\n",
    "        },\n",
    "        \"evaluation_results\": {},\n",
    "        \"test_results\": {\n",
    "            \"unit_tests_passed\": test_success if 'test_success' in locals() else False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add evaluation results if available\n",
    "    if not eval_df.empty:\n",
    "        report[\"evaluation_results\"] = {\n",
    "            \"total_evaluated\": len(eval_df),\n",
    "            \"average_similarity\": float(eval_df['similarity'].mean()),\n",
    "            \"average_response_time\": float(eval_df['response_time'].mean()),\n",
    "            \"max_similarity\": float(eval_df['similarity'].max()),\n",
    "            \"min_similarity\": float(eval_df['similarity'].min()),\n",
    "            \"success_rate\": float(sum(eval_df['is_similar']) / len(eval_df) * 100),\n",
    "            \"detailed_results\": eval_df.to_dict('records')\n",
    "        }\n",
    "    \n",
    "    return report\n",
    "\n",
    "def save_results_to_files():\n",
    "    \"\"\"Save results to various file formats\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Generate report\n",
    "    report = generate_evaluation_report()\n",
    "    \n",
    "    # Save JSON report\n",
    "    json_filename = f\"dmas_evaluation_report_{timestamp}.json\"\n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    print(f\"‚úÖ JSON report saved: {json_filename}\")\n",
    "    \n",
    "    # Save evaluation results as CSV if available\n",
    "    if not eval_df.empty:\n",
    "        csv_filename = f\"dmas_evaluation_results_{timestamp}.csv\"\n",
    "        eval_df.to_csv(csv_filename, index=False)\n",
    "        print(f\"‚úÖ Evaluation results saved: {csv_filename}\")\n",
    "    \n",
    "    # Save conversation summary as CSV if available\n",
    "    if conversations:\n",
    "        conv_summary = []\n",
    "        for i, conv in enumerate(conversations):\n",
    "            conv_summary.append({\n",
    "                \"index\": i,\n",
    "                \"sample_id\": conv.get(\"sample_id\", \"N/A\"),\n",
    "                \"speaker_a\": conv.get(\"speaker_a\", \"N/A\"),\n",
    "                \"speaker_b\": conv.get(\"speaker_b\", \"N/A\"),\n",
    "                \"sessions_count\": len([k for k in conv.get(\"sessions\", {}).keys() if k.startswith(\"session_\")])\n",
    "            })\n",
    "        \n",
    "        conv_df = pd.DataFrame(conv_summary)\n",
    "        conv_csv_filename = f\"dmas_conversations_summary_{timestamp}.csv\"\n",
    "        conv_df.to_csv(conv_csv_filename, index=False)\n",
    "        print(f\"‚úÖ Conversations summary saved: {conv_csv_filename}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and save the report\n",
    "final_report = save_results_to_files()\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ DMAS LONG-CONTEXT MEMORY EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÖ Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üìä Total conversations loaded: {len(conversations) if conversations else 0}\")\n",
    "print(f\"‚ùì Total questions loaded: {len(questions) if questions else 0}\")\n",
    "print(f\"üß™ Unit tests passed: {'‚úÖ' if test_success else '‚ùå'}\")\n",
    "\n",
    "if not eval_df.empty:\n",
    "    print(f\"üéØ Questions evaluated: {len(eval_df)}\")\n",
    "    print(f\"üìà Average similarity: {eval_df['similarity'].mean():.3f}\")\n",
    "    print(f\"‚è±Ô∏è Average response time: {eval_df['response_time'].mean():.2f}s\")\n",
    "    print(f\"üéØ Success rate: {sum(eval_df['is_similar'])/len(eval_df)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nüìÅ Files generated:\")\n",
    "print(\"- dmas_evaluation_report_[timestamp].json\")\n",
    "if not eval_df.empty:\n",
    "    print(\"- dmas_evaluation_results_[timestamp].csv\")\n",
    "if conversations:\n",
    "    print(\"- dmas_conversations_summary_[timestamp].csv\")\n",
    "\n",
    "print(\"\\nüöÄ Next steps:\")\n",
    "print(\"1. Review the generated visualizations above\")\n",
    "print(\"2. Check the saved CSV files for detailed analysis\")\n",
    "print(\"3. Compare results between different memory backends\")\n",
    "print(\"4. Run additional evaluations with different question sets\")\n",
    "print(\"5. Implement cost analysis for different memory approaches\")\n",
    "\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40afea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
