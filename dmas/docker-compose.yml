name: dmas-long-context-memory
services:
  locomo:
    build:
      context: ./locomo
      dockerfile: Dockerfile
    container_name: locomo
    ports:
      - "8002:8000"
    volumes:
      - locomo-data:/data
    environment:
      - DATA_URL=https://raw.githubusercontent.com/snap-research/locomo/refs/heads/main/data/locomo10.json
    networks:
      - dmas-network
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    restart: unless-stopped
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant-data:/qdrant/storage
    networks:
      - memory-internal
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    restart: unless-stopped
    labels:
      - "group=cloud"
  neo4j:
    image: neo4j:5.26
    container_name: neo4j
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=neo4j/password
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      - NEO4J_dbms_memory_heap_max__size=512M
      - NEO4J_dbms_memory_pagecache_size=256M
    volumes:
      - neo4j-data:/data
      - neo4j-logs:/logs
    networks:
      - memory-internal
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    restart: unless-stopped
    labels:
      - "group=cloud"
  memory:
    build:
      context: ./memory
      dockerfile: Dockerfile
    container_name: memory
    ports:
      - "8005:8002"
    depends_on:
      - qdrant
      - neo4j
    environment:
      - MEMORY_BACKEND=${MEMORY_BACKEND}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-https://api.openai.com/v1}
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=password
      - GRAPHITI_TELEMETRY_ENABLED=false
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
    networks:
      - memory-internal
      - dmas-network
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    restart: unless-stopped
    labels:
      - "group=cloud"
  responder:
    build:
      context: ./responder
      dockerfile: Dockerfile
    container_name: responder
    ports:
      - "8006:8003"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - MODEL=${MODEL:-gpt-4o}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-https://api.openai.com/v1}
    networks:
      - dmas-network
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    restart: unless-stopped
    labels:
      - "group=cloud"
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        ollama serve &
        OLLAMA_PID=$$!
        
        echo "Waiting for Ollama server to start..."
        sleep 10
        until ollama list > /dev/null 2>&1; do
          echo "Still waiting for Ollama..."
          sleep 2
        done
        
        echo "Ollama server is ready, pulling model..."
        ollama pull ${OLLAMA_MODEL}
        
        echo "Model pulled successfully"
        wait $$OLLAMA_PID
    networks:
      - dmas-network
    environment:
      - OLLAMA_MODEL=${OLLAMA_MODEL}
    healthcheck:
      test: ["CMD-SHELL", "ollama list | grep -q '${OLLAMA_MODEL}'"]
      interval: 5s
      timeout: 5s
      retries: 120
      start_period: 180s
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          memory: 6G
    restart: unless-stopped
    labels:
      - "group=edge"
  coordinator:
    build:
      context: ./coordinator
      dockerfile: Dockerfile
    container_name: coordinator
    ports:
      - "8003:8001"
    depends_on:
      locomo:
        condition: service_started
      memory:
        condition: service_started
      responder:
        condition: service_started
      ollama:
        condition: service_healthy
    environment:
      - LOCOMO_URL=http://locomo:8000
      - MEMORY_URL=http://memory:8002
      - RESPONDER_URL=http://responder:8003
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL}
    networks:
      - dmas-network
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    restart: unless-stopped
    labels:
      - "group=edge"
volumes:
  locomo-data:
  qdrant-data:
    labels:
      group: cloud
  neo4j-data:
    labels:
      group: cloud
  neo4j-logs:
    labels:
      group: cloud
  ollama_data:
    labels:
      group: edge
networks:
  dmas-network:
    driver: bridge
    name: dmas-network
  memory-internal:
    driver: bridge