services:
  openai-proxy:
    build:
      context: ./openai-proxy
      dockerfile: Dockerfile
    container_name: openai-proxy
    ports:
      - "127.0.0.1:8001:8001"
    volumes:
      - ./logs/openai:/app/logs
    restart: unless-stopped
    networks:
      - dmas-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"

  ollama-proxy:
    build:
      context: ./ollama-proxy
      dockerfile: Dockerfile
    container_name: ollama-proxy
    ports:
      - "127.0.0.1:8004:8003"
    volumes:
      - ./logs/ollama:/app/logs
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST:-http://ollama:11434}
    restart: unless-stopped
    networks:
      - dmas-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"

networks:
  dmas-network:
    external: true
